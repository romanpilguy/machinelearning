# machinelearning
<h2>Сравнение глубокой сверточной сети и многослойного перцептрона</h2>
Сети создаются на языке python с использованием библиотек Theano и Keras на платформе Google Colaborations с использованием GPU (1xTesla K80 , compute 3.7, having 2496 CUDA cores , 12GB GDDR5 VRAM)
Модели сравниваются на датасете EMNIST - наборе рукописных букв латинского алфавита

<h2>Многослойный перцептрон</h2>
<h3>Первоначальная структура</h3>
Вход - 784 нейрона,
3 скрытых слоя: 1500, 2000, 1000, 26 выходных нейрона
<pre>Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 1500)              1177500   
_________________________________________________________________
dense_2 (Dense)              (None, 2000)              3002000   
_________________________________________________________________
dropout_1 (Dropout)          (None, 2000)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              2001000   
_________________________________________________________________
dense_4 (Dense)              (None, 26)                26026     
=================================================================
Total params: 6,206,526
Trainable params: 6,206,526
Non-trainable params: 0</pre>
<h4>Первая попытка обучения</h4>
  Используются следующие параметры обучения:
  batch_size = 128
  epochs = 15
  
  Применяется дропаут 0,25 между вторым и третьим скрытыми слоями, во всех слоях используется функция активации ReLU, за исключением выходного, в котором используется Softmax. Функция потерь - перекрестная энтропия, оптимизация - градиентный спуск 
 <p> Лог обучения:</p>
 <pre>Train on 93600 samples, validate on 31200 samples
Epoch 1/15
93600/93600 [==============================] - 144s 2ms/step - loss: 0.7004 - acc: 0.7833 - val_loss: 0.4943 - val_acc: 0.8452
Epoch 2/15
93600/93600 [==============================] - 144s 2ms/step - loss: 0.3338 - acc: 0.8904 - val_loss: 0.4010 - val_acc: 0.8719
Epoch 3/15
93600/93600 [==============================] - 147s 2ms/step - loss: 0.2566 - acc: 0.9121 - val_loss: 0.3490 - val_acc: 0.8871
Epoch 4/15
93600/93600 [==============================] - 148s 2ms/step - loss: 0.2114 - acc: 0.9256 - val_loss: 0.3098 - val_acc: 0.8997
Epoch 5/15
93600/93600 [==============================] - 144s 2ms/step - loss: 0.1816 - acc: 0.9353 - val_loss: 0.2699 - val_acc: 0.9179
Epoch 6/15
93600/93600 [==============================] - 143s 2ms/step - loss: 0.1555 - acc: 0.9431 - val_loss: 0.2860 - val_acc: 0.9135
Epoch 7/15
93600/93600 [==============================] - 143s 2ms/step - loss: 0.1364 - acc: 0.9494 - val_loss: 0.2921 - val_acc: 0.9157
Epoch 8/15
93600/93600 [==============================] - 143s 2ms/step - loss: 0.1219 - acc: 0.9538 - val_loss: 0.2913 - val_acc: 0.9136
Epoch 9/15
93600/93600 [==============================] - 143s 2ms/step - loss: 0.1092 - acc: 0.9572 - val_loss: 0.3035 - val_acc: 0.9157
Epoch 10/15
93600/93600 [==============================] - 144s 2ms/step - loss: 0.1003 - acc: 0.9602 - val_loss: 0.3171 - val_acc: 0.9153
Epoch 11/15
93600/93600 [==============================] - 145s 2ms/step - loss: 0.0937 - acc: 0.9628 - val_loss: 0.3395 - val_acc: 0.9138
Epoch 12/15
93600/93600 [==============================] - 144s 2ms/step - loss: 0.0872 - acc: 0.9650 - val_loss: 0.3262 - val_acc: 0.9180
Epoch 13/15
93600/93600 [==============================] - 145s 2ms/step - loss: 0.0806 - acc: 0.9675 - val_loss: 0.3309 - val_acc: 0.9194
Epoch 14/15
93600/93600 [==============================] - 144s 2ms/step - loss: 0.0747 - acc: 0.9692 - val_loss: 0.3309 - val_acc: 0.9211
Epoch 15/15
93600/93600 [==============================] - 143s 2ms/step - loss: 0.0726 - acc: 0.9702 - val_loss: 0.3436 - val_acc: 0.9204
Test loss: 0.34356227772925285
Test accuracy: 0.920352564102564</pre>
<h2>Глубокая сверточная сеть</h2>
<h3>Первоначальная структура</h3>
<pre>Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_11 (Conv2D)           (None, 26, 26, 32)        320       
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 24, 24, 64)        18496     
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 12, 12, 64)        0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_11 (Dense)             (None, 128)               1179776   
_________________________________________________________________
dropout_12 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_12 (Dense)             (None, 26)                3354      
=================================================================
Total params: 1,201,946
Trainable params: 1,201,946
Non-trainable params: 0
_________________________________________________________________
None</pre>
<h4>Первая попытка обучения</h4>
 batch_size = 128
  epochs = 15
  Функции активации - ReLU, на выходном - Softmax
  <p>Логи:</p>
  <pre>
  Train on 93600 samples, validate on 31200 samples
Epoch 1/15
93600/93600 [==============================] - 15s 158us/step - loss: 0.8795 - acc: 0.7347 - val_loss: 0.3704 - val_acc: 0.8835
Epoch 2/15
93600/93600 [==============================] - 14s 151us/step - loss: 0.4541 - acc: 0.8590 - val_loss: 0.2716 - val_acc: 0.9119
Epoch 3/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.3755 - acc: 0.8824 - val_loss: 0.2590 - val_acc: 0.9171
Epoch 4/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.3348 - acc: 0.8931 - val_loss: 0.2462 - val_acc: 0.9197
Epoch 5/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.3091 - acc: 0.9009 - val_loss: 0.2432 - val_acc: 0.9235
Epoch 6/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.2922 - acc: 0.9056 - val_loss: 0.2222 - val_acc: 0.9304
Epoch 7/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.2753 - acc: 0.9110 - val_loss: 0.2195 - val_acc: 0.9305
Epoch 8/15
93600/93600 [==============================] - 14s 151us/step - loss: 0.2682 - acc: 0.9136 - val_loss: 0.2164 - val_acc: 0.9304
Epoch 9/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.2552 - acc: 0.9183 - val_loss: 0.2196 - val_acc: 0.9318
Epoch 10/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.2505 - acc: 0.9188 - val_loss: 0.2054 - val_acc: 0.9346
Epoch 11/15
93600/93600 [==============================] - 14s 151us/step - loss: 0.2439 - acc: 0.9216 - val_loss: 0.2135 - val_acc: 0.9339
Epoch 12/15
93600/93600 [==============================] - 14s 151us/step - loss: 0.2414 - acc: 0.9224 - val_loss: 0.2196 - val_acc: 0.9316
Epoch 13/15
93600/93600 [==============================] - 14s 151us/step - loss: 0.2362 - acc: 0.9245 - val_loss: 0.2046 - val_acc: 0.9346
Epoch 14/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.2309 - acc: 0.9251 - val_loss: 0.2093 - val_acc: 0.9342
Epoch 15/15
93600/93600 [==============================] - 14s 152us/step - loss: 0.2310 - acc: 0.9253 - val_loss: 0.2130 - val_acc: 0.9339
Test loss: 0.21300901203535688
Test accuracy: 0.9338782051282051
  </pre>
  
